{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from gensim.models import KeyedVectors\nimport re\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.parsing.preprocessing import strip_punctuation\nfrom random import randint\nimport tensorflow as tf\nfrom collections import Counter\nimport pickle",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "893a1c8aea073fd23212f34792604c2ec01e97c0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_train = pd.read_csv('../input/train.csv',sep=',')\ndf_sub = pd.read_csv('../input/test.csv',sep=',')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ecf11213833e420894208dc6c60008754aa45518",
        "trusted": true
      },
      "cell_type": "code",
      "source": "question_train_list = list(df_train['question_text'])\ntrain_label = list(df_train['target'])\nquestion_sub_list = list(df_sub['question_text'])\n\nlen_train_list = [len(sentence.split()) for sentence in question_train_list]\nlen_sub_list = [len(sentence.split()) for sentence in question_sub_list]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3d165cbf1dbea1e9370131384dbd34acef79bf2f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "numSentence_train = len(question_train_list)\nnumSentence_sub = len(question_sub_list)\nmaxSeqLength_train = max(len_train_list)\nmaxSeqLength_sub = max(len_sub_list)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "18cb84b2aedc0c821b78c138abc5294a64fe671b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "maxSeqLength = min(maxSeqLength_train, maxSeqLength_sub)\nnumSentence_train, numSentence_sub, maxSeqLength_train, maxSeqLength_sub",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "026fe07ca834a27974415e66675728a58e483bf5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\nstrip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n\ndef cleanSentences(string):\n    return re.sub(strip_special_chars, \"\", string.lower())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5c501d30dba4fbb956a7a230fa11f77d59788bfc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "clean_train_list = [cleanSentences(sentence).split()[:maxSeqLength] for sentence in question_train_list]\nclean_sub_list = [cleanSentences(sentence).split()[:maxSeqLength] for sentence in question_sub_list]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c17eae6c5dea2855890b77207ed7a00f8f636af2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "voc_to_num = {}\nnum_to_voc = {}\nindex = 0\nfor ts in clean_train_list:\n    for word in ts:\n        if not word in voc_to_num:\n            index = index +1\n            voc_to_num[word] = index\n            num_to_voc[index] = word\n\nfor ss in clean_sub_list:\n    for word in ts:\n        if not word in voc_to_num:\n            index = index +1\n            voc_to_num[word] = index\n            num_to_voc[index] = word\n            \nvoc_to_num['UNK'] = index+1\nnum_to_voc[index+1] = 'UNK'\nvoc_to_num['ZEROPAD'] = 0\nnum_to_voc[0] = 'ZEROPAD'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3bb323814fd3369cd14feeed5f1cfd345806ed7d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(voc_to_num)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2506313ba1f589b74f87d9810c55eeca6aeb9a65",
        "trusted": true
      },
      "cell_type": "code",
      "source": "wv_model =KeyedVectors.load_word2vec_format(\"../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\", binary=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a361190f84a4d545c6918331bd9239e0f7efa5e8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "embed_size = 300",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dc3793f45c7982fc49f82ab1a9b932a7e734d303",
        "trusted": true
      },
      "cell_type": "code",
      "source": "wordVectors = np.zeros((len(voc_to_num), embed_size), dtype='float32')\nwordVectors[0] = wv_model[wv_model.index2word[0]]\nfor i in range(1,len(voc_to_num)):\n    try:\n        wordVectors[i] = wv_model[num_to_voc[i]]\n    except KeyError:\n        wordVectors[i] = wv_model['UNK']\nwordVectors.shape   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "590592505b096af9b091b19dc9af37b1a7bb105f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "maxSeqLength",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3c3d3b80480bbb93e715417af52b7b0223d19ea1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "ids_t = np.zeros((numSentence_train, maxSeqLength), dtype='int32')\nfileCounter = 0\nfor tl in clean_train_list:\n    indexCounter = 0\n    lsen = len(tl)\n    for word in tl:\n        try:\n            ids_t[fileCounter][maxSeqLength-lsen+indexCounter] = voc_to_num[word]\n        except KeyError:\n            ids_t[fileCounter][maxSeqLength-lsen+indexCounter] = voc_to_num['UNK']\n        indexCounter = indexCounter + 1\n    fileCounter = fileCounter +1\n\nids_t.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "36a4899e6c6044f3156e3c322e4d1928b82bda9e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "ids_s = np.zeros((numSentence_sub, maxSeqLength), dtype='int32')\nfileCounter = 0\nfor sl in clean_sub_list:\n    indexCounter = 0\n    lsen = len(sl)\n    for word in sl:\n        try:\n            ids_s[fileCounter][maxSeqLength-lsen+indexCounter] = voc_to_num[word]\n        except KeyError:\n            ids_s[fileCounter][maxSeqLength-lsen+indexCounter] = voc_to_num['UNK']\n        indexCounter = indexCounter + 1\n    fileCounter = fileCounter +1\n\nids_s.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b58a2bade555202ea7803aa958d86d429c7cb99f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_label_arr = np.asarray(train_label, dtype=np.int32)\nsplit_frac = 0.8\nsplit_idx = int(numSentence_train*0.8)\ntrain_x, val_x = ids_t[:split_idx], ids_t[split_idx:]\ntrain_y, val_y = train_label_arr[:split_idx], train_label_arr[split_idx:]\n\ntest_idx = int(len(val_x)*0.5)\nval_x, test_x = val_x[:test_idx], val_x[test_idx:]\nval_y, test_y = val_y[:test_idx], val_y[test_idx:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aad385b0c4860b960c952cf01d501fc6c9f1e002",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"\\t\\t\\tShapes:\")\nprint(\"Train set: \\t\\t{}\".format(train_x.shape), \n      \"\\nValidation set: \\t{}\".format(val_x.shape),\n      \"\\nTest set: \\t\\t{}\".format(test_x.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "065af38793bc463f0a89c4d54ee3cf16ea1e1c46",
        "trusted": true
      },
      "cell_type": "code",
      "source": "lstm_size = 32\nlstm_layers = 1\nbatch_size = 512\nlearning_rate = 0.0005",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a04637fdcbfdd5c276f991d945dc95de130d7243",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create the graph object\ngraph = tf.Graph()\ntf.reset_default_graph()\n# Add nodes to the graph\nwith graph.as_default():\n    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n    keep_prob = tf.placeholder(tf.float32, name='keep_prob')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bc6982547e68fee6b8ed480b53e2e2af492b6bd5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Size of the embedding vectors (number of units in the embedding layer)\nwith graph.as_default():\n    embed = tf.nn.embedding_lookup(wordVectors, inputs_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "152cfd0777a118a3e31269d9bec4304fbe88e7ae",
        "trusted": true
      },
      "cell_type": "code",
      "source": "with graph.as_default():\n    \n    def get_a_cell():\n        lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n        attn = tf.contrib.rnn.AttentionCellWrapper(lstm, attn_length=20)\n        drop = tf.contrib.rnn.DropoutWrapper(attn, output_keep_prob=keep_prob)\n        return drop\n    # Your basic LSTM cell\n    # lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n    \n    # Add dropout to the cell\n    # drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n    \n    # Stack up multiple LSTM layers, for deep learning\n    cell_fw = tf.contrib.rnn.MultiRNNCell([get_a_cell() for _ in range(lstm_layers)])\n    cell_bw = tf.contrib.rnn.MultiRNNCell([get_a_cell() for _ in range(lstm_layers)])\n    #lstm_fw = tf.nn.rnn_cell.LSTMCell(lstm_size)\n    #cell_fw = tf.contrib.rnn.DropoutWrapper(lstm_fw, output_keep_prob=keep_prob)\n    \n    #lstm_bw = tf.nn.rnn_cell.LSTMCell(lstm_size)\n    #cell_bw = tf.contrib.rnn.DropoutWrapper(lstm_bw, output_keep_prob=keep_prob)\n    # Getting an initial state of all zeros\n    #initial_state_fw = cell_fw.zero_state(tf.shape(inputs_)[0], tf.float32)\n    #initial_state_bw = cell_bw.zero_state(tf.shape(inputs_)[0], tf.float32)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "81022357df78ef2fcef3e9a162b38de3623e6e21",
        "trusted": true
      },
      "cell_type": "code",
      "source": "with graph.as_default():\n    # outputs, final_state = tf.nn.dynamic_rnn(cell, embed,initial_state=initial_state)\n    #(outputs, outputs_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, embed, initial_state_fw = initial_state_fw,initial_state_bw = initial_state_bw)\n    outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, embed, dtype=tf.float32)\n    output = tf.concat(outputs,2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f24fbe15e63ae4edd28f0157304b0ffa37ce01c5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "with graph.as_default():\n    predictions = tf.contrib.layers.fully_connected(output[:, -1], 1, activation_fn=tf.sigmoid)\n    \n    cost = tf.losses.mean_squared_error(labels_, predictions)\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "113be17d5dc9ebb51f04b40cff062a6ee97cc72e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "with graph.as_default():\n    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "81db0e5411b6be4455094a5450432dca6d92737a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_batches(x, y, batch_size=100):\n    \n    n_batches = len(x)//batch_size\n    xx, yy = x[:n_batches*batch_size], y[:n_batches*batch_size]\n    #xxx, yyy = x[n_batches*batch_size:], y[n_batches*batch_size:]\n    for ii in range(0, len(xx), batch_size):\n        yield xx[ii:ii+batch_size], yy[ii:ii+batch_size]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a2529a70d0ebc0850598c45e1a1806c8bb6dcca7",
        "trusted": true
      },
      "cell_type": "code",
      "source": "epochs = 2\n\nwith graph.as_default():\n    saver = tf.train.Saver()\n\nwith tf.Session(graph=graph) as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    iteration = 1\n    for e in range(epochs):\n        #state_fw = sess.run(cell.zero_state(batch_size, tf.float32))\n        #state_bw = sess.run(cell.zero_state(batch_size, tf.float32))\n        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n            feed = {inputs_: x,\n                    labels_: y[:, None],\n                    keep_prob: 0.5\n                   }\n            loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n            \n            if iteration%5==0:\n                print(\"Epoch: {}/{}\".format(e, epochs),\n                      \"Iteration: {}\".format(iteration),\n                      \"Train loss: {:.3f}\".format(loss))\n\n            if iteration%200==0:\n                val_acc = []\n                #val_state_fw = sess.run(cell.zero_state(batch_size, tf.float32))\n                #val_state_bw = sess.run(cell.zero_state(batch_size, tf.float32))\n                for x, y in get_batches(val_x, val_y, batch_size):\n                    feed = {inputs_: x,\n                            labels_: y[:, None],\n                            keep_prob: 1\n                           }\n                    batch_acc = sess.run([accuracy], feed_dict=feed)\n                    val_acc.append(batch_acc)\n                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n            iteration +=1\n    saver.save(sess, \".ipynb_checkpoints/sentiment.ckpt\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4492470e2565e95eaa8af3127209a5fe3255ee08",
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_acc = []\nwith tf.Session(graph=graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.ipynb_checkpoints'))\n    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n        feed = {inputs_: x,\n                labels_: y[:, None],\n                keep_prob: 1\n               }\n        batch_acc = sess.run([accuracy], feed_dict=feed)\n        test_acc.append(batch_acc)\n    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "47c70fbac7391d2f4d12d3971785f592d137d7f8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "with tf.Session(graph=graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.ipynb_checkpoints'))\n    #test_state = sess.run(cell.zero_state(25000, tf.float32))\n    feed = {inputs_: ids_s[:25000],\n                keep_prob: 1}\n    test_prediction= sess.run(predictions, feed_dict=feed)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4936e79e375267e9ab79d7edd03fdd8b875d21f3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "with tf.Session(graph=graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.ipynb_checkpoints'))\n    #test_state = sess.run(cell.zero_state(31370, tf.float32))\n    feed = {inputs_: ids_s[25000:],\n                keep_prob: 1}\n    test_prediction1= sess.run(predictions, feed_dict=feed)   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9f0b93817e377364132e0a80f5f5ecc2aa819788",
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_label = sum(np.rint(test_prediction).astype(np.int32).tolist(),[])\ntest_label1 = sum(np.rint(test_prediction1).astype(np.int32).tolist(),[])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2e5d79d3090f5daf4acce705f045b3916685837d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_label_sub = test_label+test_label1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "779a071b0d1b9564615bd875fccad5f2b876efa0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(test_label_sub)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2af19e454e8c71633ac3c2f6b3222e22c19e29e7",
        "trusted": true
      },
      "cell_type": "code",
      "source": "id_list = df_sub['qid']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "75255043319e05b7a6e8e0489a1bd1184bcbcfb6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "len(id_list), test_label_sub[:10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "62d969ea9624426205817147d4f66c8ea5013fe0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "sub_dict={}\nsub_dict['qid'] = id_list\nsub_dict['prediction'] = test_label_sub\nsub_df = pd.DataFrame.from_dict(sub_dict)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f276b60904455d52b2e3e0d6fd7e9ede7af96822",
        "trusted": true
      },
      "cell_type": "code",
      "source": "sub_df.to_csv('submission.csv',index=False)\nsub_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7da0d394532c96267e8706171b0178190160a957",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(os.listdir(\"../working\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65b98a5e667295390812a74c3e5964d56aa72bbe"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}